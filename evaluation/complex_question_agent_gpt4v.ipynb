{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将视觉语言模型gpt4v封装成API服务，通过agent对complex question进行解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan-and-execute agents accomplish an objective by first planning what to do, then executing the sub tasks. This idea is largely inspired by [BabyAGI](https://github.com/yoheinakajima/babyagi) and then the [\"Plan-and-Solve\" paper](https://arxiv.org/abs/2305.04091). The planning is almost always done by an LLM. The execution is usually done by a separate agent (equipped with tools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain.memory import ConversationTokenBufferMemory, ConversationSummaryBufferMemory\n",
    "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
    "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
    "from langchain_experimental.plan_and_execute import (\n",
    "    PlanAndExecute,\n",
    "    load_agent_executor,\n",
    "    load_chat_planner,\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from PIL import Image\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "import sys\n",
    "\n",
    "os.environ[\"HTTP_PROXY\"] = \"http://10.16.64.223:7890\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://10.16.64.223:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "def gpt4v_video(video_path, q):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    base64Frames = []\n",
    "    while video.isOpened():\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            break\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        base64Frames.append(base64.b64encode(buffer).decode(\"utf-8\"))\n",
    "\n",
    "    video.release()\n",
    "\n",
    "    total_frames = len(base64Frames)  # 获取列表总长度\n",
    "    interval = total_frames // 16  # 计算间隔\n",
    "\n",
    "    # 使用切片操作抽取帧\n",
    "    selected_frames = base64Frames[::interval]\n",
    "\n",
    "    PROMPT_MESSAGES = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                q,\n",
    "                *map(lambda x: {\"image\": x}, selected_frames),\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    params = {\n",
    "        \"model\": \"gpt-4-vision-preview\",\n",
    "        \"messages\": PROMPT_MESSAGES,\n",
    "        \"max_tokens\": 4090,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        result = client.chat.completions.create(**params)\n",
    "        return result.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"An exception occurred: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTObjectDetectingTool(BaseTool):\n",
    "    name = \"GPT Object Detecting Tool\"\n",
    "    description = \"Utilize this tool for object detection in images or videos. Input the image or video path and ask questions related to object detection. \"\\\n",
    "                    \"The tool will provide information about the detected objects in the image or video. \"\\\n",
    "                    \"Keep in mind that GPT may have limitations in object detection, and the results should be verified for accuracy. \"\\\n",
    "                    \"If the response is 'I cannot assist with this request,' consider refining your question or retrying.\"\n",
    "\n",
    "\n",
    "\n",
    "    def _run(self, video_path, question):\n",
    "        # model = YOLO('yolov8n.pt')\n",
    "        response = gpt4v_video(video_path, question)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTObjectTrackingTool(BaseTool):\n",
    "    name = \"GPT Object Tracking Tool\"\n",
    "    description = \"Utilize this tool for object tracking in videos. Input the video path and ask questions related to object tracking. \" \\\n",
    "                \"The tool will provide information about the tracked objects in the video. \" \\\n",
    "                \"Keep in mind that GPT may have limitations in object tracking, and the results should be verified for accuracy. \" \\\n",
    "                \"If the response is 'I cannot assist with this request,' consider refining your question or retrying.\"\n",
    "\n",
    "\n",
    "\n",
    "    def _run(self, video_path, question):\n",
    "        # model = YOLO('yolov8n.pt')\n",
    "        response = gpt4v_video(video_path, question)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTActionRecognitionTool(BaseTool):\n",
    "    name = \"GPT Action Recognition Tool\"\n",
    "    description = \"Utilize this tool for video action recognition. Input the video path and ask questions related to action recognition within the video. \"\\\n",
    "                    \"The tool will provide information about the recognized actions or activities in the video. \"\\\n",
    "                    \"Keep in mind that GPT may have limitations in video action recognition, and the results should be verified for accuracy. \"\\\n",
    "                    \"If the response is 'I cannot assist with this request,' consider refining your question or retrying.\"\n",
    "\n",
    "\n",
    "    def _run(self, video_path, question):\n",
    "        # model = YOLO('yolov8n.pt')\n",
    "        response = gpt4v_video(video_path, question)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTVideoFaceRecognitionTool(BaseTool):\n",
    "    name = \"GPT Video Face Recognition Tool\"\n",
    "    description = \"Use this tool to perform face recognition on a video. Provide the path to the video along with any questions related to face recognition. \" \\\n",
    "                \"The tool will return information about recognized faces in the video. \" \\\n",
    "                \"Keep in mind that GPT might make mistakes, so it's advisable to verify critical information. \" \\\n",
    "                \"If the response is 'I cannot assist with this request,' consider retrying.\"\n",
    "\n",
    "    def _run(self, video_path, question):\n",
    "        response = gpt4v_video(video_path, question)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTVideoClassificationTool(BaseTool):\n",
    "    name = \"GPT Video Classification Tool\"\n",
    "    description = \"Utilize this tool for video classification tasks. Input the video path and ask questions related to video classification. \"\\\n",
    "                    \"The tool will provide information about the classified categories or labels for the video content. \"\\\n",
    "                    \"Keep in mind that GPT may have limitations in video classification, and the results should be verified for accuracy. \"\\\n",
    "                    \"If the response is 'I cannot assist with this request,' consider refining your question or retrying.\"\n",
    "\n",
    "    def _run(self, video_path, question):\n",
    "        response = gpt4v_video(video_path, question)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTVideoSegmentationTool(BaseTool):\n",
    "    name = \"GPT Video Segmentation Tool\"\n",
    "    description = \"This tool is designed for video segmentation. Provide the video path and ask questions related to video segmentation. \" \\\n",
    "                \"The tool will return information about segmented regions or objects in the video. \" \\\n",
    "                \"Please note that GPT may have limitations in video segmentation, and results should be validated for accuracy. \" \\\n",
    "                \"If the response is 'I cannot assist with this request,' consider refining your question or retrying.\"\n",
    "\n",
    "\n",
    "    def _run(self, video_path, question):\n",
    "        # model = YOLO('yolov8n.pt')\n",
    "        response = gpt4v_video(video_path, question)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTVideoCaptioningTool(BaseTool):\n",
    "    name = \"GPT Video Captioning Tool\"\n",
    "    description = \"Utilize this tool for video captioning tasks. Input the video path and ask questions related to generating captions for the video content. \"\\\n",
    "                    \"The tool will provide descriptive captions or textual summaries for the video. \"\\\n",
    "                    \"Keep in mind that GPT may have limitations in video captioning, and the results should be verified for accuracy. \"\\\n",
    "                    \"If the response is 'I cannot assist with this request,' consider refining your question or retrying.\"\n",
    "\n",
    "\n",
    "    def _run(self, video_path, question):\n",
    "        response = gpt4v_video(video_path, question)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTVideoSummarizationTool(BaseTool):\n",
    "    name = \"GPT Video Summarization Tool\"\n",
    "    description = \"Use this tool to generate a summary of a video. Input the video path and ask questions related to video content summarization. \" \\\n",
    "                \"The tool will provide a concise summary based on the input video. \" \\\n",
    "                \"Keep in mind that GPT may have limitations, and the generated summary should be verified for accuracy. \" \\\n",
    "                \"If the response is 'I cannot assist with this request,' consider refining your question or retrying.\"\n",
    "\n",
    "\n",
    "    def _run(self, video_path, question):\n",
    "        response = gpt4v_video(video_path, question)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTVideoPoseEstimationTool(BaseTool):\n",
    "    name = \"GPT Video Pose Estimation Tool\"\n",
    "    description = \"Use this tool when given the path to a video that you would like to estimate the people's poses in the video. \" \\\n",
    "                  \"You should input the video path and a question about Video Pose Estimation. It will return the corresponding answer.\" \\\n",
    "                  \"It will return people's poses in the video. \" \\\n",
    "                  \"Please note that GPT can make mistakes. Consider checking important information.\" \\\n",
    "                  \"If it says 'I cannot assist with this request', you may consider retrying.\"\n",
    "\n",
    "    def _run(self, video_path, question):\n",
    "        # model = YOLO('yolov8n.pt')\n",
    "        response = gpt4v_video(video_path, question)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _arun(self, query: str):\n",
    "        raise NotImplementedError(\"This tool does not support async\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"<input your api key>\"\n",
    "search = DuckDuckGoSearchAPIWrapper()\n",
    "llm = ChatOpenAI(temperature=0, api_key=OPENAI_API_KEY, model = \"gpt-4\")\n",
    "llm_math_chain = LLMMathChain.from_llm(llm=llm, verbose=True)\n",
    "tools = [GPTVideoPoseEstimationTool(),GPTVideoFaceRecognitionTool(),GPTVideoSummarizationTool(),GPTObjectTrackingTool(),GPTVideoSegmentationTool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(temperature=0, api_key=OPENAI_API_KEY, model='gpt-4')\n",
    "planner = load_chat_planner(model)\n",
    "executor = load_agent_executor(model, tools, verbose=True)\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "# agent = PlanAndExecute(planner=planner, executor=executor)\n",
    "agent = initialize_agent(\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, # AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    tools=tools,\n",
    "    llm=model,\n",
    "    max_iterations=5,\n",
    "    verbose=True,\n",
    "    memory=conversational_memory,\n",
    "    early_stopping_method='generate'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'video_path': '/home/nkd/Documents/ssd_nvme0n1/jiajiyuan/ai_agent/video/star/Charades_v1_480/WBS4I.mp4',\n",
       " 'question': 'Question: What happened before the person watched at the book?\\nOptions:\\n(A) Put down the sandwich.\\n(B) Washed the table.\\n(C) Opened the closet/cabinet.\\n(D) Put down the pillow.',\n",
       " 'gt': '(C) Opened the closet/cabinet.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('./mvbech_file/complex.json', 'r') as file:\n",
    "    complex_questions = json.load(file)\n",
    "complex_questions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 137, 'video_path': '/home/nkd/Documents/ssd_nvme0n1/jiajiyuan/ai_agent/video/star/Charades_v1_480/P2HZG.mp4', 'question': 'Question: What happened before the person opened the closet/cabinet?\\nOptions:\\n(A) Closed the laptop.\\n(B) Put down the cup/glass/bottle.\\n(C) Put down the food.\\n(D) Threw the bag.', 'gt': '(A) Closed the laptop.'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The user wants to know what action was performed before the person opened the closet/cabinet in the video. I will use the GPT Video Summarization tool to get a summary of the video, which should help answer the question.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"GPT Video Summarization\",\n",
      "  \"action_input\": {\n",
      "    \"video_path\": \"/home/nkd/Documents/ssd_nvme0n1/jiajiyuan/ai_agent/video/star/Charades_v1_480/P2HZG.mp4\",\n",
      "    \"question\": \"What happened before the person opened the closet/cabinet?\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mIt seems like the person was initially sitting at a cluttered table and then rose from their chair to open the closet or cabinet. There are various items on the table that indicate they might have been working on something, perhaps an art project or some kind of DIY activity, as suggested by the presence of a paint palette and what looks like ceramic or craft items. The person appears to be in a kitchen or a workspace with kitchen amenities, and they seem to be looking for something or need to put something away in the closet/cabinet.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mBased on the video summary, it seems that the person was sitting at a cluttered table before they rose to open the closet or cabinet. However, the summary does not specify whether the person closed a laptop, put down a cup/glass/bottle, put down food, or threw a bag. To provide a more accurate answer, I will use the GPT Object Tracking Tool to identify the last object the person interacted with before opening the closet/cabinet.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"GPT Object Tracking Tool\",\n",
      "  \"action_input\": {\n",
      "    \"video_path\": \"/home/nkd/Documents/ssd_nvme0n1/jiajiyuan/ai_agent/video/star/Charades_v1_480/P2HZG.mp4\",\n",
      "    \"question\": \"What object did the person interact with before opening the closet/cabinet?\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIt seems like the sequence of images shows a person moving around a kitchen and interacting with various objects. Before opening the cabinet/closet, the person appears to have interacted with something on the counter, but due to the image quality and motion blur, it's not clear what the object specifically is. It could be a container or some kitchen utensil, but I cannot provide a precise description.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe video summary and object tracking results do not provide a clear answer to the question. The person was sitting at a cluttered table before they rose to open the closet or cabinet, and they interacted with something on the counter, but it's not clear what the object specifically is. None of the options provided by the user (closed a laptop, put down a cup/glass/bottle, put down food, or threw a bag) are clearly supported by the information I have. I will provide the information I have and explain the uncertainty.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The person was sitting at a cluttered table before they rose to open the closet or cabinet, and they interacted with something on the counter, but it's not clear what the object specifically is. None of the options (closed a laptop, put down a cup/glass/bottle, put down food, or threw a bag) are clearly supported by the information I have.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "question = complex_questions[random.randint(0, len(complex_questions) - 1)]\n",
    "video_path = question.get(\"video_path\")\n",
    "user_question = question.get(\"question\")\n",
    "print(question)\n",
    "response = agent.run(f'{user_question}, this is the video path: {video_path}')\n",
    "\n",
    "# for question in complex_questions:\n",
    "#     video_path = question.get(\"video_path\")\n",
    "#     user_question = question.get(\"question\")\n",
    "#     print(question)\n",
    "#     response = agent.run(f'{user_question}, this is the video path: {video_path}')\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
